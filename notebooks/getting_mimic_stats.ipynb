{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eeaa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8f2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.py\n",
    "\n",
    "class mimicforsetlstm():\n",
    "    def __init__(self, datadir, configdir, name, device, normalize=False, padding=False, load_all=True,num_of_instances=20, subsample=False, top=False):\n",
    "\n",
    "        self.datadir = datadir\n",
    "        self.configdir = configdir\n",
    "        self.name = name\n",
    "        self.device  = device\n",
    "        self.do_padding = padding\n",
    "        self.load_all = load_all\n",
    "        self.normalize = normalize\n",
    "        # The max length for IHM is 1569 after removing the 32 patients as done in SeFT\n",
    "        self.MAX_LEN = 1569 \n",
    "        with open(self.configdir+ 'discretizer_config.json','r') as json_file:\n",
    "            self.discretizer_config = json.loads(json_file.read())\n",
    "        \n",
    "        with open(self.configdir + 'channel_info.json','r') as json_file:\n",
    "            self.channelinfo = json.loads(json_file.read())\n",
    "        ignore_files = []\n",
    "        with open('../ignore_file.txt','r') as fp:\n",
    "            for line in fp:\n",
    "                ignore_files.append(line.strip())\n",
    "            \n",
    "        self.subsample = subsample\n",
    "        self.top = top\n",
    "        #shifted one index\n",
    "        #top_five from desc\n",
    "        self.topfive_idx = [12,13,9,14,2,11]\n",
    "        #bottom_five from aesc\n",
    "        self.bottom_idx = [1, 10, 16, 3, 17, 6]\n",
    "        \n",
    "        self.df = pd.read_csv(self.datadir + f'{name}_listfile.csv')\n",
    "        print(\"originaldflen:\", len(self.df))\n",
    "        self.df = self.df[~self.df.stay.isin(ignore_files)]\n",
    "        print(\"afterdeldflen:\", len(self.df))\n",
    "        \n",
    "        if self.load_all:\n",
    "            self.df = self.df[:]\n",
    "        else:\n",
    "            self.df = self.df[:num_of_instances]\n",
    "        \n",
    "        \n",
    "\n",
    "        self.id_to_channel = self.discretizer_config['id_to_channel']\n",
    "        self.is_categorical_channel = self.discretizer_config['is_categorical_channel']\n",
    "\n",
    "        self.possible_values = self.channelinfo\n",
    "        \n",
    "        self.normal_values = self.discretizer_config['normal_values']\n",
    "        print(self.normal_values.keys())\n",
    "        #self.configdir = configdir\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __labels__(self):\n",
    "        return list(self.df.y_true)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        info = self.df.iloc[idx,:].values\n",
    "        filename = info[0]\n",
    "        y = info[1]\n",
    "        if self.name != 'test':\n",
    "            patient_df = pd.read_csv(self.datadir + f'train/{filename}')\n",
    "        else:\n",
    "            patient_df = pd.read_csv(self.datadir + f'test/{filename}')\n",
    "        \n",
    "        #print(\"original col:\", patient_df.columns)\n",
    "        if self.subsample:\n",
    "            if self.top:\n",
    "                patient_df = patient_df.iloc[:,[0] +self.topfive_idx]\n",
    "            else:\n",
    "                patient_df = patient_df.iloc[:,[0] +self.bottom_idx]\n",
    "                \n",
    "        info = filename.split('_')\n",
    "        patient_id = float(info[0] + '.' +  info[1].split('episode')[-1])\n",
    "        feature_dict = {fname:idx for idx, fname in enumerate(patient_df.columns[1:])}\n",
    "        \n",
    "        feature_count = {idx:0 for idx, fname in enumerate(patient_df.columns[1:])}\n",
    "        feature_ftime = {idx:[] for idx, fname in enumerate(patient_df.columns[1:])}\n",
    "        time_global = []\n",
    "        type_global = []\n",
    "        z_global = []\n",
    "        delt_global = []\n",
    "        \n",
    "        all_timestamps = patient_df.Hours.values\n",
    "        #print('total timestamps', len(all_timestamps))\n",
    "        if all_timestamps[-1] != all_timestamps[0]:\n",
    "            duration = all_timestamps[-1] - all_timestamps[0]\n",
    "        else:\n",
    "            duration = all_timestamps[0]\n",
    "\n",
    "        for i,rowidx in enumerate(patient_df.iterrows()):\n",
    "            info = rowidx[1]\n",
    "            \n",
    "            timestamp = info.values[0]\n",
    "            features = info.values[1:] # feature values\n",
    "            \n",
    "            rowmask = ~info.isna().values[1:] # which are available\n",
    "         \n",
    "            available_feature_keys = np.array(list(feature_dict.keys()))[rowmask]\n",
    "            available_feature_values = features[rowmask]\n",
    "            \n",
    "            for af,av in zip(available_feature_keys, available_feature_values):\n",
    "                feat_key = feature_dict[af]\n",
    "                \n",
    "                time_global.append(timestamp)\n",
    "                type_global.append(feat_key)\n",
    "                \n",
    "                feature_count[feat_key] +=1\n",
    "                \n",
    "                if self.is_categorical_channel[af]:\n",
    "                    \"fixing categorical values\"\n",
    "                    vdict = self.possible_values[af]['values']\n",
    "                    \n",
    "                    if str(av) not in vdict:\n",
    "                        \n",
    "                        av = str(av).split('.')[0]\n",
    "                        z_global.append(vdict[str(av)])\n",
    "                    else:\n",
    "                        z_global.append(vdict[str(av)])\n",
    "                else:\n",
    "                    z_global.append(av)\n",
    "                \n",
    "                if len(feature_ftime[feat_key]) == 0:\n",
    "                    # meaning: feature af is appearing for the first time\n",
    "                    feature_ftime[feat_key].append(timestamp)\n",
    "                    delt_global.append(0)\n",
    "                else:\n",
    "                    delt_global.append(timestamp - feature_ftime[feat_key][0])\n",
    "\n",
    "        curr_len = len(z_global)\n",
    "        if self.do_padding:\n",
    "            diff = self.MAX_LEN - curr_len\n",
    "            if diff > 0:\n",
    "                time_global += [-1]*diff\n",
    "                type_global += [-1]*diff\n",
    "                z_global += [-1]*diff\n",
    "                delt_global += [-1]*diff\n",
    "            else:\n",
    "                time_global = time_global[:self.MAX_LEN]\n",
    "                type_global = type_global[:self.MAX_LEN]\n",
    "                z_global = z_global[:self.MAX_LEN]\n",
    "                delt_global = delt_global[:self.MAX_LEN]\n",
    "                \n",
    "        \n",
    "        stacked_x = torch.vstack((torch.tensor(time_global),\n",
    "                                    torch.tensor(type_global),\n",
    "                                    torch.tensor(z_global), \n",
    "                                torch.tensor(delt_global)))\n",
    "\n",
    "        return {'x': stacked_x.to(self.device),\n",
    "                'y':torch.tensor(y).to(self.device),\n",
    "                 'lx':torch.tensor(curr_len).to(self.device),\n",
    "                'pid':torch.tensor(patient_id).to(self.device)}, list(feature_count.values()), duration, patient_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e97c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "\n",
    "trainset = mimicforsetlstm(datadir = './data/in-hospital-mortality/', \n",
    "                           configdir = './mimic3models/resources/',\n",
    "                            name='train',  load_all=True, num_of_instances=10996,\n",
    "                            device=device, subsample=False, top=False,\n",
    "                          padding=True)\n",
    "\n",
    "\n",
    "#print(trainset.name,':' , trainset.__len__())\n",
    "all_patient_sampling = []\n",
    "ds = []\n",
    "ys = []\n",
    "diag_times = []\n",
    "trainmiss, trainall = 0,0\n",
    "c = 0\n",
    "save_for_slan = []\n",
    "for sample,_, d,pdf in tqdm(iter(trainset)):\n",
    "    x = sample['x']\n",
    "    y = sample['y']\n",
    "    all_patient_sampling.append(_)\n",
    "    ds.append(d)\n",
    "    ys.append(y.detach().cpu().tolist())\n",
    "    r,c = pdf.iloc[:,1:].shape\n",
    "    trainall +=r*c\n",
    "    trainmiss += (sum(sum(np.array(pdf.iloc[:,1:].isna()))))\n",
    "    diag_times.append(len(pdf.Hours.values))\n",
    "    save_for_slan.append(list(sum(np.array(pdf.iloc[:,1:].isna()))) + [r] + [y.detach().cpu().tolist()])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28352a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagn_per_hour = 1/np.array(ds)[:,None] * np.array(all_patient_sampling)#num of patient x nul of feature\n",
    "\n",
    "print(diagn_per_hour.shape)\n",
    "np.mean(diagn_per_hour, axis=0)# mean across variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e3927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check missingness\n",
    "\n",
    "#(14604887, 9865879)(4246396, 2857945)(4263175, 2877824)\n",
    "#a,y = (14604887+4246396+4263175, 9865879+2857945+2877824)\n",
    "\n",
    "#y,a,a-y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9737a5",
   "metadata": {},
   "source": [
    "# Stat file generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class normalizer():\n",
    "    def __init__(self, datadir, configdir, name, device,num_of_instances=20, normalize=False, padding=False, load_all=True):\n",
    "\n",
    "        self.datadir = datadir\n",
    "        self.configdir = configdir\n",
    "        self.name = name\n",
    "        self.device  = device\n",
    "        self.do_padding = padding\n",
    "        self.load_all = load_all\n",
    "        self.num_of_instances = num_of_instances\n",
    "        self.normalize = normalize\n",
    "        # The max length for IHM is 1569 after removing the 32 patients as done in SeFT\n",
    "        self.MAX_LEN = 1569 \n",
    "        with open(self.configdir+ 'discretizer_config.json','r') as json_file:\n",
    "            self.discretizer_config = json.loads(json_file.read())\n",
    "        \n",
    "        with open(self.configdir + 'channel_info.json','r') as json_file:\n",
    "            self.channelinfo = json.loads(json_file.read())\n",
    "        ignore_files = []\n",
    "        \n",
    "        with open('../ignore_file.txt','r') as fp:\n",
    "            for line in fp:\n",
    "                ignore_files.append(line.strip())\n",
    "\n",
    "        self.df = pd.read_csv(self.datadir + f'{name}_listfile.csv')\n",
    "        self.df = self.df[~self.df.stay.isin(ignore_files)]\n",
    "        \n",
    "        if self.load_all:\n",
    "            self.df = self.df[:]\n",
    "        else:\n",
    "            self.df = self.df[:num_of_instances]\n",
    "\n",
    "        self.df = self.df[~self.df.stay.isin(ignore_files)]\n",
    "\n",
    "        self.id_to_channel = self.discretizer_config['id_to_channel']\n",
    "        self.is_categorical_channel = self.discretizer_config['is_categorical_channel']\n",
    "\n",
    "        self.possible_values = self.channelinfo\n",
    "        \n",
    "        self.normal_values = self.discretizer_config['normal_values']\n",
    "        print(self.normal_values)\n",
    "        \n",
    "        self.var_stat = self._normalize_data(self.df, debug=False)\n",
    "        \n",
    "    def sum_max_min(self, vec):\n",
    "        \n",
    "        isnan = [~np.isnan(v) for v in vec]\n",
    "        #print(isnan, sum(isnan))\n",
    "        sm = np.nansum(vec)\n",
    "        mn = np.inf\n",
    "        for v in vec:\n",
    "            mn = min(mn,v)\n",
    "        mx = -np.inf\n",
    "        for v in vec:\n",
    "            mx = max(mx,v)\n",
    "            \n",
    "        return sm,mx, mn, sum(isnan)\n",
    "        \n",
    "        \n",
    "    def _normalize_data(self, patient_df, debug=False):\n",
    "        \n",
    "        print('hllo')\n",
    "        \n",
    "        is_categorical = []\n",
    "        nvariables = 0\n",
    "        features_names = list(self.normal_values.keys())\n",
    "        print(features_names)\n",
    "        feat_dict = {f: {'min':np.inf, 'max':-np.inf, 'sum':0, 'count':0, 'is_cat':False, 'value':[]} for f in features_names}\n",
    "        \n",
    "        for idx in tqdm(range(len(self.df))):\n",
    "            info = self.df.iloc[idx,:].values\n",
    "            #print(info)\n",
    "            filename = info[0]\n",
    "            if self.name != 'test':\n",
    "                patient_df = pd.read_csv(self.datadir + f'train/{filename}')\n",
    "            else:\n",
    "                patient_df = pd.read_csv(self.datadir + f'test/{filename}')\n",
    "                \n",
    "            max_values = []\n",
    "            min_values = []\n",
    "            sum_values = []\n",
    "            count_values = []\n",
    "            \n",
    "            \n",
    "            info = patient_df.iloc[:,1:].to_numpy().T\n",
    "            #print(info.shape)\n",
    "            \n",
    "            for feat, featn in zip(info, features_names):\n",
    "                \n",
    "                if self.is_categorical_channel[featn]:\n",
    "                    mask = [~np.isnan(v) if not isinstance(v,str) else True for v in feat]\n",
    "                    if debug:\n",
    "                        print(\"categorical pass done\")\n",
    "                        print(feat)\n",
    "                        print(sum([~np.isnan(v) if not isinstance(v,str) else True for v in feat]))\n",
    "                    #s,mx,mn, c = -2,-2,-2, sum([~np.isnan(v) if not isinstance(v,str) else True for v in feat])\n",
    "                else:\n",
    "                    mask = [~np.isnan(v) for v in feat]\n",
    "                    if debug:\n",
    "                        1;#print(feat)\n",
    "                        #print(featn)\n",
    "                    #print(np.isnan(feat))\n",
    "                    #s,mx,mn, c = self.sum_max_min(feat)\n",
    "                    \n",
    "                #print('==nonnan==>',feat[mask])\n",
    "                if len(feat[mask]) > 0:\n",
    "                    if not self.is_categorical_channel[featn]:\n",
    "                        feat_dict[featn]['sum'] += sum(feat[mask])\n",
    "                        feat_dict[featn]['count'] += len(feat[mask])\n",
    "                        feat_dict[featn]['is_cat'] = self.is_categorical_channel[featn]\n",
    "                        feat_dict[featn]['min'] = min(feat_dict[featn]['min'], min(feat[mask]))\n",
    "                        feat_dict[featn]['max'] = max(feat_dict[featn]['max'], max(feat[mask]))\n",
    "                        feat_dict[featn]['value'].extend(feat[mask])\n",
    "                    else:\n",
    "                        feat_dict[featn]['count'] += len(feat[mask])\n",
    "                        feat_dict[featn]['is_cat'] = self.is_categorical_channel[featn]\n",
    "                        feat_dict[featn]['value'].extend(feat[mask])\n",
    "                        \n",
    "    \n",
    "        return feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "            \n",
    "train = normalizer(datadir = './data/in-hospital-mortality/', \n",
    "                           configdir = './mimic3models/resources/',\n",
    "                            name='train',  load_all=True, num_of_instances=10996,\n",
    "                            device=device,\n",
    "                          padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc5150e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stat_dict= {}\n",
    "\n",
    "for k in train.var_stat:\n",
    "    #print(k)\n",
    "    final_stat_dict[k] = {'is_cat': None, 'mean':None, 'std':None, 'min':None, 'max':None}\n",
    "    if not train.var_stat[k]['is_cat']:\n",
    "        final_stat_dict[k]['is_cat'] = train.var_stat[k]['is_cat']\n",
    "        final_stat_dict[k]['min'] = train.var_stat[k]['min']\n",
    "        final_stat_dict[k]['max'] = train.var_stat[k]['max']\n",
    "        info = train.var_stat[k]\n",
    "        if info['count']>0:\n",
    "            #print(info)\n",
    "            print('mean:', info['sum']/info['count'])\n",
    "            print('std:', np.std(info['value']))\n",
    "            print(\"*\"*20)\n",
    "            final_stat_dict[k]['mean'] = info['sum']/info['count']\n",
    "            final_stat_dict[k]['std'] = np.std(info['value'])\n",
    "            \n",
    "    else:\n",
    "        final_stat_dict[k]['is_cat'] = train.var_stat[k]['is_cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ce16bd",
   "metadata": {},
   "source": [
    "#  Writing mimic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4121d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('mimic_stat.json', 'w') as fp:\n",
    "    json.dump(final_stat_dict, fp )\n",
    "    \n",
    "    \n",
    "with open('sampling_rate_mimic.npy', 'wb') as f:\n",
    "    np.save(f,np.mean(diagn_per_hour, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc62260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
